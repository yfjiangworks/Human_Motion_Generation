![Image](resources/fig1.png)


# Abstract

Recently, skeleton-based human action has become a hot research topic because the compact representation of human skeletons brings new blood to this research domain. As a result, researchers began to notice the importance of using RGB or other sensors to analyze human action by extracting skeleton information. Leveraging the rapid development of deep learning (DL), a significant number of skeleton-based human action approaches have been presented with fine-designed DL structures recently. However, a well-trained DL model always demands high-quality and sufficient data, which is hard to obtain without costing high expenses and human labor. In this paper, we introduce a novel data augmentation method for skeleton-based action recognition tasks, which can effectively generate high-quality and diverse sequential actions. In order to obtain natural and realistic action sequences, we propose a denoising diffusion probabilistic models (DDPMs) that can generate a series of synthetic action sequences, and its generation process is precisely guided by a spatial-temporal transformer (ST-Trans). Experimental results show that our method outperforms the state-of-the-art (SOTA) motion generation approaches on different naturality and diversity metrics. It proves that its high-quality synthetic data can also be effectively deployed to existing action recognition models with significant performance improvement. 

# Overview

![Image](resources/fig2.png)
<p align="center">
We introduce the proposed method in this section with details. In Figure \ref{fig3}, we demonstrate the overview of the proposed method. Given a class condition $y$, we aim to generate natural action sequences using a pretrained diffusion model $\theta$ under the guidance of a pretrained spatial-temporal transformer $\phi$. We start from a pretrained diffusion model with two inputs: a class condition $y$ and a noise map $x_t$ sampled from Gaussian distribution $\mathcal{N}(0,\textbf{I})$. Next, a series of intermediate latent $x_{t-1}...x_1$ are sampled step by step. At each sampling step, the process is guided by the pretrained transformer $p_\phi(y|\hat{x}_0)$ using its gradient $\nabla_{\hat{x}_0}p_\phi$ after acquiring the clean estimation $\hat{x}_0$ of the noisy latent $x_t$. So that this guiding mechanism leads the sampling process gradually toward the class condition $y$. The final output is a synthetic skeleton image, and this skeleton image representation is then translated to 3D joint coordinates and can be easily used by action recognition methods.
</p>

In this paper, we propose a cGAN-based COVID-19 CT image synthesis method. Here, COVID-19 CT image synthesis is formulated as a semantic-layout-conditional image-to-image translation task. The structure consisting of two main components: a global-local generator and a multi-resolution discriminator. During the training stage, the semantic segmentation map of a corresponding CT image is passed to the global-local generator, where the label information from the segmentation map is extracted via down-sampling and re-rendered to generate a synthesized image via up-sampling. The segmentation map is then concatenated with the corresponding CT image or synthesized CT image to form the input for the multi-resolution discriminator, which is used to distinguish the input as either real or synthesized. The decisions from the discriminator are used to calculate the loss and update the parameters for both the generator and discriminator. During the testing stage, only the generator is involved. A data augmented segmentation map is used as input for the generator, from which a realistic synthesized image can be obtained after extraction and re-rendering. This synthesized lung CT image is then combined with the non-lung area to form a completely synthesized CT image as the final result. Above figure presents an overview of the proposed method.

# Synthetic COVID-19 CT samples
![Image](resources/fig3.png)
<p align="center">
Synthetic lung CT images generated by the proposed method and the other two competitive state-of-the-art image synthesis approaches. The first column shows the segmentation map including the lung (red), ground-glass opacity (blue), and consolidation (green) areas. The second column shows the original CT image. The third, fourth, fifth columns show the synthetic samples which are generated by the proposed method, SEAN and SPADE in order. Each case is presented with zoom in order to show more details, and the yellow arrows point out the special area which is described in the main text.
</p>

![Image](resources/fig4.png)
<p align="center">
Synthetic lung CT images generated by the proposed method. Eight samples are selected, each from an individual patient. The first column shows the segmentation map including the lung (red), ground-glass opacity (blue), and consolidation (green) areas. The second and third columns show the original and synthetic CT images, respectively. The synthetic CT images here merge the synthetic lung CT image and the corresponding real non-lung area. The fourth and fifth columns depict CT images for the original lung and synthesized CT images, respectively.
</p>

# Acknowlegements
This research work is supported by a National Research Foundation (NRF) grant funded by the MSIP of Korea (number 2019R1A2C2009480).

# Citation
```
@article{jiang2020covid,
  title={COVID-19 CT image synthesis with a conditional generative adversarial network},
  author={Jiang, Yifan and Chen, Han and Loew, Murray and Ko, Hanseok},
  journal={IEEE Journal of Biomedical and Health Informatics},
  volume={25},
  number={2},
  pages={441--452},
  year={2020},
  publisher={IEEE}
}
```
